{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Corpus for training"
      ],
      "metadata": {
        "id": "VudTDXUbgdwn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0E7Gn5flsW6",
        "outputId": "3248c672-5f81-4e0a-ef52-8a99197a4b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Corpus:\n",
            "This is the first document.\n",
            "This document is the second document.\n",
            "And this is the third one.\n",
            "Is this the first document?\n"
          ]
        }
      ],
      "source": [
        "corpus = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "print(\"Training Corpus:\")\n",
        "for doc in corpus:\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Vocabulary"
      ],
      "metadata": {
        "id": "u9EryoBDgm4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_chars = set()\n",
        "for doc in corpus:\n",
        "  for char in doc:\n",
        "    unique_chars.add(char)\n",
        "\n",
        "vocab = list(unique_chars)\n",
        "vocab.sort()\n",
        "\n",
        "end_of_word = \"</w>\"\n",
        "vocab.append(end_of_word)\n",
        "\n",
        "print(\"Initial Vocabulary:\")\n",
        "print(vocab)\n",
        "print(f\"Vocabulary Size: {len(vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vs1ElL9l52f",
        "outputId": "5cb6d7e7-3ff1-4c75-bb21-a63c17dda911"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Vocabulary:\n",
            "[' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>']\n",
            "Vocabulary Size: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Pre-tokenized words with their respective frequencies"
      ],
      "metadata": {
        "id": "RwVO5xePgxMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_splits={}\n",
        "for doc in corpus:\n",
        "  words = doc.split(' ')\n",
        "  for word in words:\n",
        "    if word:\n",
        "      char_list = list(word) + [end_of_word]\n",
        "      word_tuple = tuple(char_list)\n",
        "      if word_tuple not in word_splits:\n",
        "        word_splits[word_tuple] = 0\n",
        "      word_splits[word_tuple] += 1\n",
        "\n",
        "print(\"\\nPre-tokenized Word Frequencies:\")\n",
        "print(word_splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5uISBozmW91",
        "outputId": "7ebaa1c8-6215-4bf3-fe4f-deb15a2e7712"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pre-tokenized Word Frequencies:\n",
            "{('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper function to get the word pairs with the highest frequencies"
      ],
      "metadata": {
        "id": "mPz1GWM7hBg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "def get_pair_stats(splits):\n",
        "  pair_counts = collections.defaultdict(int)\n",
        "  for word_tuple, freq in splits.items():\n",
        "    symbols  = list(word_tuple)\n",
        "    for i in range(len(symbols) - 1):\n",
        "      pair = (symbols[i], symbols[i+1])\n",
        "      pair_counts[pair] += freq\n",
        "  return pair_counts"
      ],
      "metadata": {
        "id": "Ex4DyjNsnIVY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper function to merge the pairs with the highest frequencies"
      ],
      "metadata": {
        "id": "ZZKiXtOshOKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_pair(pair_to_merge, splits):\n",
        "  new_splits= {}\n",
        "\n",
        "  (first, second) = pair_to_merge\n",
        "  merged_token = first + second\n",
        "\n",
        "  for word_tuple, freq in splits.items():\n",
        "    symbols = list(word_tuple)\n",
        "    new_symbols = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(symbols):\n",
        "\n",
        "      if i < len(symbols) - 1 and symbols[i] == first and symbols[i+1] == second:\n",
        "        new_symbols.append(merged_token)\n",
        "\n",
        "        i += 2\n",
        "      else:\n",
        "        new_symbols.append(symbols[i])\n",
        "\n",
        "        i += 1\n",
        "    new_splits[tuple(new_symbols)] = freq\n",
        "  return new_splits"
      ],
      "metadata": {
        "id": "GErx4Hnnrl7m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the tokenizer for 15 epochs"
      ],
      "metadata": {
        "id": "RUlop_19hcj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_merges = 15\n",
        "merges={}\n",
        "\n",
        "current_splits = word_splits.copy()\n",
        "\n",
        "print(\"\\n--- Starting BPE Merges ---\")\n",
        "print(f\"Initial Splits: {current_splits}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "for i in range(num_merges):\n",
        "  print(f\"\\nMerge Iteration {i+1}/{num_merges}\")\n",
        "\n",
        "  pair_stats = get_pair_stats(current_splits)\n",
        "  if not pair_stats:\n",
        "    print(\"No more pairs to merge\")\n",
        "    break\n",
        "\n",
        "  sorted_pairs = sorted(pair_stats.items(), key = lambda item: item[1], reverse=True)\n",
        "  print(f\"Top 5 Pair Frequencies: {sorted_pairs[:5]}\")\n",
        "\n",
        "\n",
        "  best_pair = max(pair_stats, key = pair_stats.get)\n",
        "  best_freq = pair_stats[best_pair]\n",
        "  print(f'Found Best Pair: {best_pair} with Frequency: {best_freq}')\n",
        "\n",
        "\n",
        "  current_splits = merge_pair(best_pair, current_splits)\n",
        "  new_token = best_pair[0] + best_pair[1]\n",
        "  print(f\"Merging {best_pair} into '{new_token}'\")\n",
        "  print(f\"Splits after merge: {current_splits}\")\n",
        "\n",
        "\n",
        "  vocab.append(new_token)\n",
        "  print(f'Updated Vocabulary: {vocab}')\n",
        "\n",
        "\n",
        "  merges[best_pair] = new_token\n",
        "  print(f\"Updated Merges: {merges}\")\n",
        "\n",
        "  print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5471yEdKrW73",
        "outputId": "c0208366-d421-4e85-b853-109d6d9e5f90"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting BPE Merges ---\n",
            "Initial Splits: {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
            "------------------------------\n",
            "\n",
            "Merge Iteration 1/15\n",
            "Top 5 Pair Frequencies: [(('s', '</w>'), 8), (('i', 's'), 7), (('t', 'h'), 7), (('h', 'i'), 5), (('h', 'e'), 4)]\n",
            "Found Best Pair: ('s', '</w>') with Frequency: 8\n",
            "Merging ('s', '</w>') into 's</w>'\n",
            "Splits after merge: {('T', 'h', 'i', 's</w>'): 2, ('i', 's</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
            "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>']\n",
            "Updated Merges: {('s', '</w>'): 's</w>'}\n",
            "------------------------------\n",
            "\n",
            "Merge Iteration 2/15\n",
            "Top 5 Pair Frequencies: [(('i', 's</w>'), 7), (('t', 'h'), 7), (('h', 'i'), 5), (('h', 'e'), 4), (('e', '</w>'), 4)]\n",
            "Found Best Pair: ('i', 's</w>') with Frequency: 7\n",
            "Merging ('i', 's</w>') into 'is</w>'\n",
            "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'is</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
            "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>']\n",
            "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>'}\n",
            "------------------------------\n",
            "\n",
            "Merge Iteration 3/15\n",
            "Top 5 Pair Frequencies: [(('t', 'h'), 7), (('h', 'is</w>'), 4), (('h', 'e'), 4), (('e', '</w>'), 4), (('d', 'o'), 4)]\n",
            "Found Best Pair: ('t', 'h') with Frequency: 7\n",
            "Merging ('t', 'h') into 'th'\n",
            "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('th', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
            "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th']\n",
            "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th'}\n",
            "------------------------------\n",
            "\n",
            "Merge Iteration 4/15\n",
            "Top 5 Pair Frequencies: [(('th', 'e'), 4), (('e', '</w>'), 4), (('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4)]\n",
            "Found Best Pair: ('th', 'e') with Frequency: 4\n",
            "Merging ('th', 'e') into 'the'\n",
            "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
            "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the']\n",
            "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the'}\n",
            "------------------------------\n",
            "\n",
            "Merge Iteration 5/15\n",
            "Top 5 Pair Frequencies: [(('the', '</w>'), 4), (('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4)]\n",
            "Found Best Pair: ('the', '</w>') with Frequency: 4\n",
            "Merging ('the', '</w>') into 'the</w>'\n",
            "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
            "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>']\n",
            "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>'}\n",
            "------------------------------\n",
            "\n",
            "Merge Iteration 6/15\n",
            "Top 5 Pair Frequencies: [(('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4)]\n",
            "Found Best Pair: ('d', 'o') with Frequency: 4\n",
            "Merging ('d', 'o') into 'do'\n",
            "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('do', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('do', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('do', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
            "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do']\n",
            "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do'}\n",
            "------------------------------\n",
            "\n",
            "Merge Iteration 7/15\n",
            "Top 5 Pair Frequencies: [(('do', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4)]\n",
            "Found Best Pair: ('do', 'c') with Frequency: 4\n",
            "Merging ('do', 'c') into 'doc'\n",
            "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('doc', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('doc', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('doc', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
            "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc']\n",
            "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc'}\n",
            "------------------------------\n",
            "\n",
            "Merge Iteration 8/15\n",
            "Top 5 Pair Frequencies: [(('doc', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4)]\n",
            "Found Best Pair: ('doc', 'u') with Frequency: 4\n",
            "Merging ('doc', 'u') into 'docu'\n",
            "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('docu', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('docu', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('docu', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
            "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu']\n",
            "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu'}\n",
            "------------------------------\n",
            "\n",
            "Merge Iteration 9/15\n",
            "Top 5 Pair Frequencies: [(('docu', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3)]\n",
            "Found Best Pair: ('docu', 'm') with Frequency: 4\n",
            "Merging ('docu', 'm') into 'docum'\n",
            "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('docum', 'e', 'n', 't', '.', '</w>'): 2, ('docum', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('docum', 'e', 'n', 't', '?', '</w>'): 1}\n",
            "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum']\n",
            "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum'}\n",
            "------------------------------\n",
            "\n",
            "Merge Iteration 10/15\n",
            "Top 5 Pair Frequencies: [(('docum', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3)]\n",
            "Found Best Pair: ('docum', 'e') with Frequency: 4\n",
            "Merging ('docum', 'e') into 'docume'\n",
            "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('docume', 'n', 't', '.', '</w>'): 2, ('docume', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('docume', 'n', 't', '?', '</w>'): 1}\n",
            "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume']\n",
            "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume'}\n",
            "------------------------------\n",
            "\n",
            "Merge Iteration 11/15\n",
            "Top 5 Pair Frequencies: [(('docume', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3), (('.', '</w>'), 3)]\n",
            "Found Best Pair: ('docume', 'n') with Frequency: 4\n",
            "Merging ('docume', 'n') into 'documen'\n",
            "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('documen', 't', '.', '</w>'): 2, ('documen', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('documen', 't', '?', '</w>'): 1}\n",
            "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen']\n",
            "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen'}\n",
            "------------------------------\n",
            "\n",
            "Merge Iteration 12/15\n",
            "Top 5 Pair Frequencies: [(('documen', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3), (('.', '</w>'), 3), (('d', '</w>'), 3)]\n",
            "Found Best Pair: ('documen', 't') with Frequency: 4\n",
            "Merging ('documen', 't') into 'document'\n",
            "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('document', '.', '</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
            "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document']\n",
            "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document'}\n",
            "------------------------------\n",
            "\n",
            "Merge Iteration 13/15\n",
            "Top 5 Pair Frequencies: [(('i', 'r'), 3), (('.', '</w>'), 3), (('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2)]\n",
            "Found Best Pair: ('i', 'r') with Frequency: 3\n",
            "Merging ('i', 'r') into 'ir'\n",
            "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.', '</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
            "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir']\n",
            "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir'}\n",
            "------------------------------\n",
            "\n",
            "Merge Iteration 14/15\n",
            "Top 5 Pair Frequencies: [(('.', '</w>'), 3), (('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2), (('f', 'ir'), 2)]\n",
            "Found Best Pair: ('.', '</w>') with Frequency: 3\n",
            "Merging ('.', '</w>') into '.</w>'\n",
            "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd', '</w>'): 1, ('o', 'n', 'e', '.</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
            "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>']\n",
            "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir', ('.', '</w>'): '.</w>'}\n",
            "------------------------------\n",
            "\n",
            "Merge Iteration 15/15\n",
            "Top 5 Pair Frequencies: [(('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2), (('f', 'ir'), 2), (('ir', 's'), 2)]\n",
            "Found Best Pair: ('d', '</w>') with Frequency: 3\n",
            "Merging ('d', '</w>') into 'd</w>'\n",
            "Splits after merge: {('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd</w>'): 1, ('A', 'n', 'd</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd</w>'): 1, ('o', 'n', 'e', '.</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
            "Updated Vocabulary: [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>', 's</w>', 'is</w>', 'th', 'the', 'the</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'ir', '.</w>', 'd</w>']\n",
            "Updated Merges: {('s', '</w>'): 's</w>', ('i', 's</w>'): 'is</w>', ('t', 'h'): 'th', ('th', 'e'): 'the', ('the', '</w>'): 'the</w>', ('d', 'o'): 'do', ('do', 'c'): 'doc', ('doc', 'u'): 'docu', ('docu', 'm'): 'docum', ('docum', 'e'): 'docume', ('docume', 'n'): 'documen', ('documen', 't'): 'document', ('i', 'r'): 'ir', ('.', '</w>'): '.</w>', ('d', '</w>'): 'd</w>'}\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- BPE Merges Complete ---\")\n",
        "print(f\"Final Vocabulary Size: {len(vocab)}\")\n",
        "print(\"\\nLearned Merges (Pair -> New Token):\")\n",
        "# Pretty print merges\n",
        "for pair, token in merges.items():\n",
        "    print(f\"{pair} -> '{token}'\")\n",
        "\n",
        "print(\"\\nFinal Word Splits after all merges:\")\n",
        "print(current_splits)\n",
        "\n",
        "print(\"\\nFinal Vocabulary (sorted):\")\n",
        "# Sort for consistent viewing\n",
        "final_vocab_sorted = sorted(list(set(vocab))) # Use set to remove potential duplicates if any step introduced them\n",
        "print(final_vocab_sorted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcaa0BPWS4rB",
        "outputId": "db016aea-8403-4990-9b36-5597510fe433"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- BPE Merges Complete ---\n",
            "Final Vocabulary Size: 35\n",
            "\n",
            "Learned Merges (Pair -> New Token):\n",
            "('s', '</w>') -> 's</w>'\n",
            "('i', 's</w>') -> 'is</w>'\n",
            "('t', 'h') -> 'th'\n",
            "('th', 'e') -> 'the'\n",
            "('the', '</w>') -> 'the</w>'\n",
            "('d', 'o') -> 'do'\n",
            "('do', 'c') -> 'doc'\n",
            "('doc', 'u') -> 'docu'\n",
            "('docu', 'm') -> 'docum'\n",
            "('docum', 'e') -> 'docume'\n",
            "('docume', 'n') -> 'documen'\n",
            "('documen', 't') -> 'document'\n",
            "('i', 'r') -> 'ir'\n",
            "('.', '</w>') -> '.</w>'\n",
            "('d', '</w>') -> 'd</w>'\n",
            "\n",
            "Final Word Splits after all merges:\n",
            "{('T', 'h', 'is</w>'): 2, ('is</w>',): 3, ('the</w>',): 4, ('f', 'ir', 's', 't', '</w>'): 2, ('document', '.</w>'): 2, ('document', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd</w>'): 1, ('A', 'n', 'd</w>'): 1, ('th', 'is</w>'): 2, ('th', 'ir', 'd</w>'): 1, ('o', 'n', 'e', '.</w>'): 1, ('I', 's</w>'): 1, ('document', '?', '</w>'): 1}\n",
            "\n",
            "Final Vocabulary (sorted):\n",
            "[' ', '.', '.</w>', '</w>', '?', 'A', 'I', 'T', 'c', 'd', 'd</w>', 'do', 'doc', 'docu', 'docum', 'docume', 'documen', 'document', 'e', 'f', 'h', 'i', 'ir', 'is</w>', 'm', 'n', 'o', 'r', 's', 's</w>', 't', 'th', 'the', 'the</w>', 'u']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assigning every token an ID"
      ],
      "metadata": {
        "id": "pry8e6qphk2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token2id = {token: idx for idx, token in enumerate(final_vocab_sorted)}\n",
        "print(token2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gshMDl-UF-q",
        "outputId": "581b6abc-7489-4ed6-b8de-60642d94f9f5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, '.': 1, '.</w>': 2, '</w>': 3, '?': 4, 'A': 5, 'I': 6, 'T': 7, 'c': 8, 'd': 9, 'd</w>': 10, 'do': 11, 'doc': 12, 'docu': 13, 'docum': 14, 'docume': 15, 'documen': 16, 'document': 17, 'e': 18, 'f': 19, 'h': 20, 'i': 21, 'ir': 22, 'is</w>': 23, 'm': 24, 'n': 25, 'o': 26, 'r': 27, 's': 28, 's</w>': 29, 't': 30, 'th': 31, 'the': 32, 'the</w>': 33, 'u': 34}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper function to apply BPE to every word."
      ],
      "metadata": {
        "id": "Y4lLhnX2hz3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_bpe(word, merges, end_of_word=\"</w>\"):\n",
        "    symbols = list(word) + [end_of_word]\n",
        "\n",
        "    while True:\n",
        "        pairs = [(symbols[i], symbols[i+1]) for i in range(len(symbols) - 1)]\n",
        "        pair_freqs = {pair: idx for idx, pair in enumerate(pairs) if pair in merges}\n",
        "\n",
        "        if not pair_freqs:\n",
        "            break\n",
        "\n",
        "        best_pair = min(pair_freqs, key=lambda pair: pair_freqs[pair])\n",
        "\n",
        "        new_symbols = []\n",
        "        i = 0\n",
        "        while i < len(symbols):\n",
        "            if i < len(symbols) - 1 and (symbols[i], symbols[i+1]) == best_pair:\n",
        "                new_symbols.append(symbols[i] + symbols[i+1])\n",
        "                i += 2\n",
        "            else:\n",
        "                new_symbols.append(symbols[i])\n",
        "                i += 1\n",
        "        symbols = new_symbols\n",
        "    return symbols\n"
      ],
      "metadata": {
        "id": "_1Y8BHiPhzQ9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer to encode text to tokens"
      ],
      "metadata": {
        "id": "6oLE_c2wiR7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_tokenizer(corpus, merges):\n",
        "  encoded_corpus_ids = []\n",
        "\n",
        "  for doc in corpus:\n",
        "      token_ids = []\n",
        "      for word in doc.strip().split():\n",
        "          bpe_tokens = apply_bpe(word, merges)\n",
        "          for token in bpe_tokens:\n",
        "              token_id = token2id.get(token)\n",
        "              if token_id is not None:\n",
        "                  token_ids.append(token_id)\n",
        "      encoded_corpus_ids.append(token_ids)\n",
        "  return encoded_corpus_ids"
      ],
      "metadata": {
        "id": "DZTOC5y5iK34"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder to get the actual text"
      ],
      "metadata": {
        "id": "Rzjn2Teqitr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2token = {v: k for k, v in token2id.items()}\n",
        "\n",
        "def decode_token_ids(token_ids):\n",
        "    tokens = [id2token[token_id] for token_id in token_ids]\n",
        "    text = ''.join(tokens).replace('</w>', ' ').strip()  # if you use '</w>' for spaces\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "ZvQ_jWvbiVrI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GCLjKbwgiyr-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}