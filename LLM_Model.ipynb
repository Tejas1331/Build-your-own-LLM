{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "fKPVRap7jK2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y6UkkY_jCNa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the training dataset"
      ],
      "metadata": {
        "id": "p7sI_bD8jPW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 5\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for token_seq in encoded_corpus_ids:\n",
        "    if len(token_seq) < context_size + 1:\n",
        "        continue\n",
        "    for i in range(len(token_seq) - context_size):\n",
        "        X.append(token_seq[i:i + context_size])\n",
        "        y.append(token_seq[i + context_size])\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.long)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "TYWxULtMjUmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the model architecture"
      ],
      "metadata": {
        "id": "ymjS3HICjZeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, context_size, embed_dim=64, n_heads=2, ff_dim=128):\n",
        "        super(MiniGPT, self).__init__()\n",
        "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embed = nn.Embedding(context_size, embed_dim)\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads=n_heads, batch_first=True)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.output_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        token_embeddings = self.token_embed(x)\n",
        "        positions = torch.arange(T, device=x.device).unsqueeze(0)\n",
        "        pos_embeddings = self.pos_embed(positions)\n",
        "        x = token_embeddings + pos_embeddings\n",
        "\n",
        "        attn_output, _ = self.attn(x, x, x, need_weights=False)\n",
        "        x = self.ln1(x + attn_output)\n",
        "\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.ln2(x + ff_output)\n",
        "\n",
        "        logits = self.output_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "3UXhHPncjYtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "GbtwqflOjkRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(token2id)\n",
        "model = MiniGPT(vocab_size=vocab_size, context_size=context_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        logits = logits[:, -1, :]  # Only last token\n",
        "        loss = loss_fn(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(loader):.4f}\")"
      ],
      "metadata": {
        "id": "i5J2QMIljoMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating the output"
      ],
      "metadata": {
        "id": "VBILKkhyjtep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, seed_tokens, max_new_tokens=4):\n",
        "    model.eval()\n",
        "    tokens = seed_tokens[:]\n",
        "    for _ in range(max_new_tokens):\n",
        "        x = torch.tensor(tokens[-context_size:], dtype=torch.long).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
        "            tokens.append(next_token)\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "WOUx2Tsqjt74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model"
      ],
      "metadata": {
        "id": "FcOCguHLj-IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = [\"Is this the \"]\n",
        "seed_tokens = gpt_tokenizer(input_text, merges)\n",
        "\n",
        "# Ensure correct length\n",
        "if len(seed_tokens) < context_size:\n",
        "    seed_tokens_new = [0] * (context_size - len(seed_tokens))\n",
        "    seed_tokens_new.extend(seed_tokens[0])\n",
        "    seed_tokens = seed_tokens_new\n",
        "else:\n",
        "    seed_tokens = seed_tokens[-context_size:]\n",
        "\n",
        "# Generate and decode output\n",
        "generated_ids = generate(model, seed_tokens, max_new_tokens=20)\n",
        "print(generated_ids)\n",
        "output_text = decode_token_ids(generated_ids)\n",
        "print(\"Input Prompt:\", input_text)\n",
        "print(\"Generated Output:\", output_text)"
      ],
      "metadata": {
        "id": "i6GVirfcj-ii"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}